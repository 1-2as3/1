"""å…¨é¢æ·±åº¦æ£€æŸ¥ï¼šæ¨¡å‹ç»“æ„ã€å‚æ•°ã€é…ç½®ä¸€è‡´æ€§"""
import sys
import torch
from mmengine.config import Config
from mmdet.utils import register_all_modules
from mmdet.registry import MODELS, DATASETS
from copy import deepcopy
import json

print("=" * 80)
print("æ·±åº¦æ¨¡å‹æ£€æŸ¥ï¼šå‘ç°éšè—ç¼ºé™·")
print("=" * 80)

register_all_modules(init_default_scope=True)

cfg = Config.fromfile('configs/llvip/stage2_kaist_domain_ft.py')
base_cfg = Config.fromfile('configs/_base_/models/faster_rcnn_r50_fpn.py')

def _deep_merge(dst: dict, src: dict):
    for k, v in src.items():
        if isinstance(v, dict) and isinstance(dst.get(k), dict):
            _deep_merge(dst[k], v)
        else:
            dst[k] = v
    return dst

merged = _deep_merge(deepcopy(base_cfg['model']), cfg.model)
roi = merged.get('roi_head', {})
if 'use_domain_loss' in roi:
    roi.pop('use_domain_loss')

# æ„å»ºæ¨¡å‹
model = MODELS.build(merged)

print("\n" + "=" * 80)
print("æ£€æŸ¥ 1: é…ç½®æ–‡ä»¶ä¸€è‡´æ€§")
print("=" * 80)

issues = []

# 1.1 æ£€æŸ¥ load_from è·¯å¾„
load_from = cfg.get('load_from', None)
print(f"\n1.1 é¢„è®­ç»ƒæƒé‡è·¯å¾„: {load_from}")
if load_from:
    import os
    if not os.path.exists(load_from):
        issues.append(f"âŒ é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {load_from}")
        print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè®­ç»ƒå°†ä»å¤´å¼€å§‹ï¼")
    else:
        print(f"   âœ“ æ–‡ä»¶å­˜åœ¨")
else:
    issues.append("âš ï¸ æœªè®¾ç½® load_fromï¼Œå°†ä»å¤´è®­ç»ƒï¼ˆå¯èƒ½ä¸ç¬¦åˆ Stage2 é¢„æœŸï¼‰")
    print("   âš ï¸ æœªè®¾ç½® load_from")

# 1.2 æ£€æŸ¥å­¦ä¹ ç‡ä¸å†»ç»“ç­–ç•¥
optim_cfg = cfg.get('optim_wrapper', {})
lr = optim_cfg.get('optimizer', {}).get('lr', None)
paramwise = optim_cfg.get('paramwise_cfg', {})
custom_keys = paramwise.get('custom_keys', {})

print(f"\n1.2 å­¦ä¹ ç‡ä¸å†»ç»“ç­–ç•¥:")
print(f"   åŸºç¡€å­¦ä¹ ç‡: {lr}")
if 'backbone' in custom_keys:
    bb_mult = custom_keys['backbone'].get('lr_mult', 1.0)
    print(f"   Backbone lr_mult: {bb_mult}")
    if bb_mult != 0.0:
        issues.append(f"âš ï¸ Backbone æœªå®Œå…¨å†»ç»“ (lr_mult={bb_mult})ï¼ŒStage2 åº”è¯¥å†»ç»“")
        print(f"   âš ï¸ Backbone æœªå®Œå…¨å†»ç»“ï¼")
    else:
        print(f"   âœ“ Backbone å·²å†»ç»“")
else:
    issues.append("âš ï¸ æœªè®¾ç½® backbone å­¦ä¹ ç‡å€ç‡ï¼Œé»˜è®¤ä¸å†»ç»“")
    print("   âš ï¸ æœªé…ç½® backbone å†»ç»“")

# 1.3 æ£€æŸ¥è®­ç»ƒè½®æ•°
max_epochs = cfg.get('train_cfg', {}).get('max_epochs', None)
if max_epochs is None:
    # æ£€æŸ¥ scheduler
    scheduler = cfg.get('param_scheduler', {})
    if isinstance(scheduler, dict):
        max_epochs = scheduler.get('T_max', None)
print(f"\n1.3 è®­ç»ƒè½®æ•°: {max_epochs}")
if max_epochs and max_epochs < 10:
    issues.append(f"âš ï¸ è®­ç»ƒè½®æ•°è¿‡å°‘ ({max_epochs})ï¼Œå¯èƒ½ä¸è¶³ä»¥æ”¶æ•›")
    print(f"   âš ï¸ è½®æ•°å¯èƒ½ä¸è¶³")

print("\n" + "=" * 80)
print("æ£€æŸ¥ 2: æ¨¡å‹å‚æ•°çŠ¶æ€")
print("=" * 80)

# 2.1 ç»Ÿè®¡æ‰€æœ‰å‚æ•°
total_params = 0
trainable_params = 0
frozen_params = 0

param_groups = {
    'backbone': [],
    'neck': [],
    'rpn': [],
    'roi_head': [],
    'msp': [],
    'macl': [],
    'other': []
}

for name, param in model.named_parameters():
    total_params += param.numel()
    if param.requires_grad:
        trainable_params += param.numel()
    else:
        frozen_params += param.numel()
    
    # åˆ†ç±»å‚æ•°
    if 'backbone' in name:
        param_groups['backbone'].append((name, param.numel(), param.requires_grad))
    elif 'neck' in name:
        if 'msp' in name.lower():
            param_groups['msp'].append((name, param.numel(), param.requires_grad))
        else:
            param_groups['neck'].append((name, param.numel(), param.requires_grad))
    elif 'rpn' in name:
        param_groups['rpn'].append((name, param.numel(), param.requires_grad))
    elif 'roi_head' in name:
        if 'macl' in name.lower():
            param_groups['macl'].append((name, param.numel(), param.requires_grad))
        else:
            param_groups['roi_head'].append((name, param.numel(), param.requires_grad))
    else:
        param_groups['other'].append((name, param.numel(), param.requires_grad))

print(f"\n2.1 å‚æ•°æ€»è§ˆ:")
print(f"   æ€»å‚æ•°: {total_params:,} ({total_params/1e6:.2f}M)")
print(f"   å¯è®­ç»ƒ: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
print(f"   å†»ç»“çš„: {frozen_params:,} ({frozen_params/1e6:.2f}M)")

print(f"\n2.2 å„æ¨¡å—å‚æ•°åˆ†å¸ƒ:")
for group_name, params_list in param_groups.items():
    if not params_list:
        continue
    group_total = sum(p[1] for p in params_list)
    group_trainable = sum(p[1] for p in params_list if p[2])
    group_frozen = group_total - group_trainable
    print(f"\n   {group_name.upper()}:")
    print(f"     æ€»è®¡: {group_total:,} ({group_total/1e6:.2f}M)")
    print(f"     å¯è®­ç»ƒ: {group_trainable:,} ({group_trainable/1e6:.2f}M)")
    print(f"     å†»ç»“: {group_frozen:,} ({group_frozen/1e6:.2f}M)")
    
    # æ£€æŸ¥å¼‚å¸¸
    if group_name == 'backbone':
        if group_trainable > 0:
            issues.append(f"âŒ Backbone æœ‰ {group_trainable:,} ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œåº”è¯¥å…¨éƒ¨å†»ç»“ï¼")
            print(f"     âŒ å‘ç°å¯è®­ç»ƒå‚æ•°ï¼ˆåº”å…¨éƒ¨å†»ç»“ï¼‰")
            # åˆ—å‡ºå‰5ä¸ªå¯è®­ç»ƒçš„
            trainable = [p for p in params_list if p[2]][:5]
            for name, num, _ in trainable:
                print(f"        - {name}: {num:,}")
    
    elif group_name in ['msp', 'macl']:
        if group_trainable == 0:
            issues.append(f"âŒ {group_name.upper()} æ¨¡å—æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")
            print(f"     âŒ æ— å¯è®­ç»ƒå‚æ•°ï¼ˆæ¨¡å—å¤±æ•ˆï¼‰")
    
    # æ˜¾ç¤ºå‰3ä¸ªå‚æ•°ç¤ºä¾‹
    if params_list:
        print(f"     ç¤ºä¾‹å‚æ•°:")
        for name, num, grad in params_list[:3]:
            grad_str = "âœ“å¯è®­ç»ƒ" if grad else "âœ—å†»ç»“"
            print(f"       - {name}: {num:,} ({grad_str})")

print("\n" + "=" * 80)
print("æ£€æŸ¥ 3: æ¨¡å—å®ä¾‹åŒ–çŠ¶æ€")
print("=" * 80)

# 3.1 æ£€æŸ¥å…³é”®æ¨¡å—å­˜åœ¨æ€§
print(f"\n3.1 å…³é”®æ¨¡å—:")
checks = {
    'backbone': (hasattr(model, 'backbone'), type(model.backbone).__name__ if hasattr(model, 'backbone') else 'N/A'),
    'neck': (hasattr(model, 'neck'), type(model.neck).__name__ if hasattr(model, 'neck') else 'N/A'),
    'neck.msp_module': (hasattr(model.neck, 'msp_module') if hasattr(model, 'neck') else False, 
                        type(model.neck.msp_module).__name__ if (hasattr(model, 'neck') and hasattr(model.neck, 'msp_module')) else 'N/A'),
    'rpn_head': (hasattr(model, 'rpn_head'), type(model.rpn_head).__name__ if hasattr(model, 'rpn_head') else 'N/A'),
    'roi_head': (hasattr(model, 'roi_head'), type(model.roi_head).__name__ if hasattr(model, 'roi_head') else 'N/A'),
    'roi_head.macl_head': (hasattr(model.roi_head, 'macl_head') if hasattr(model, 'roi_head') else False,
                           type(model.roi_head.macl_head).__name__ if (hasattr(model, 'roi_head') and hasattr(model.roi_head, 'macl_head')) else 'N/A'),
}

for name, (exists, type_name) in checks.items():
    status = "âœ“" if exists else "âœ—"
    print(f"   {status} {name}: {type_name}")
    if not exists and '.' in name:
        parent = name.rsplit('.', 1)[0]
        issues.append(f"âŒ {name} ä¸å­˜åœ¨")

# 3.2 æ£€æŸ¥ MSP å’Œ MACL çš„é…ç½®å‚æ•°
if hasattr(model.neck, 'msp_module'):
    msp = model.neck.msp_module
    print(f"\n3.2 MSP æ¨¡å—é…ç½®:")
    print(f"   channels: {getattr(msp, 'channels', 'N/A')}")
    print(f"   reduction: {getattr(msp, 'reduction', 'N/A')}")
    alpha_val = getattr(msp, 'alpha', None)
    if alpha_val is not None:
        if isinstance(alpha_val, torch.nn.Parameter):
            print(f"   alpha (å¯å­¦ä¹ ): åˆå§‹å€¼={alpha_val.item():.4f}")
        else:
            print(f"   alpha (å›ºå®š): {alpha_val}")

if hasattr(model.roi_head, 'macl_head'):
    macl = model.roi_head.macl_head
    print(f"\n3.3 MACL æ¨¡å—é…ç½®:")
    print(f"   in_dim: {getattr(macl, 'in_dim', 'N/A')}")
    print(f"   proj_dim: {getattr(macl, 'proj_dim', 'N/A')}")
    tau_val = getattr(macl, 'tau', None)
    if tau_val is not None:
        if isinstance(tau_val, torch.nn.Parameter):
            print(f"   tau (å¯å­¦ä¹ ): åˆå§‹å€¼={tau_val.item():.4f}")
        else:
            print(f"   tau (å›ºå®š): {tau_val}")
    print(f"   use_dhn: {getattr(macl, 'use_dhn', 'N/A')}")
    if hasattr(macl, 'dhn_sampler') and macl.dhn_sampler:
        dhn = macl.dhn_sampler
        print(f"   DHN queue_size: {getattr(dhn, 'queue_size', 'N/A')}")
        print(f"   DHN momentum: {getattr(dhn, 'momentum', 'N/A')}")

print("\n" + "=" * 80)
print("æ£€æŸ¥ 4: æ•°æ®é›†é…ç½®")
print("=" * 80)

# 4.1 æ£€æŸ¥è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
for split_name in ['train', 'val', 'test']:
    split_key = f'{split_name}_dataloader'
    ds_cfg = None
    if split_key in cfg and cfg[split_key] is not None:
        if isinstance(cfg[split_key], dict):
            ds_cfg = cfg[split_key].get('dataset', {})
        else:
            ds_cfg = getattr(cfg[split_key], 'dataset', {}) if hasattr(cfg[split_key], 'dataset') else {}
    elif 'data' in cfg and split_name in cfg.data:
        ds_cfg = cfg.data[split_name]
    
    if not ds_cfg:
        print(f"\n4.{['train','val','test'].index(split_name)+1} {split_name.upper()} æ•°æ®é›†:")
        print(f"   âœ— æœªé…ç½®")
        if split_name == 'train':
            issues.append(f"âŒ ç¼ºå°‘è®­ç»ƒé›†é…ç½®")
        continue
    
    print(f"\n4.{['train','val','test'].index(split_name)+1} {split_name.upper()} æ•°æ®é›†:")
    print(f"   type: {ds_cfg.get('type', 'N/A')}")
    print(f"   data_root: {ds_cfg.get('data_root', 'N/A')}")
    print(f"   ann_file: {ds_cfg.get('ann_file', 'N/A')}")
    print(f"   return_modality_pair: {ds_cfg.get('return_modality_pair', 'N/A')}")
    
    # æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§
    import os
    data_root = ds_cfg.get('data_root', '')
    ann_file = ds_cfg.get('ann_file', '')
    if data_root and not os.path.exists(data_root):
        issues.append(f"âŒ {split_name} æ•°æ®é›† data_root ä¸å­˜åœ¨: {data_root}")
        print(f"   âŒ data_root ä¸å­˜åœ¨")
    if ann_file and not os.path.exists(ann_file):
        issues.append(f"âŒ {split_name} æ•°æ®é›† ann_file ä¸å­˜åœ¨: {ann_file}")
        print(f"   âŒ ann_file ä¸å­˜åœ¨")
    
    # æ£€æŸ¥ return_modality_pair
    pair_mode = ds_cfg.get('return_modality_pair', False)
    if pair_mode:
        issues.append(f"âš ï¸ {split_name} æ•°æ®é›†å¯ç”¨äº† return_modality_pair=Trueï¼Œè¿™ä¼šè·³è¿‡æ ‡å‡† pipeline")
        print(f"   âš ï¸ å¯ç”¨äº†é…å¯¹æ¨¡å¼ï¼ˆå¯èƒ½ä¸å…¼å®¹æ ‡å‡†è®­ç»ƒï¼‰")

print("\n" + "=" * 80)
print("æ£€æŸ¥ 5: æŸå¤±å‡½æ•°é…ç½®")
print("=" * 80)

# 5.1 æ£€æŸ¥ RoI Head çš„æŸå¤±æƒé‡
if hasattr(model.roi_head, 'lambda1'):
    print(f"\n5.1 æŸå¤±æƒé‡:")
    print(f"   lambda1 (MACL): {model.roi_head.lambda1}")
    print(f"   lambda2 (DHN): {model.roi_head.lambda2}")
    print(f"   lambda3 (Domain): {model.roi_head.lambda3}")
    
    if model.roi_head.lambda1 == 0:
        issues.append("âš ï¸ MACL æŸå¤±æƒé‡ä¸º0ï¼Œæ¨¡å—å°†ä¸å‚ä¸è®­ç»ƒ")
        print(f"   âš ï¸ lambda1=0ï¼ŒMACL æŸå¤±è¢«ç¦ç”¨")

print("\n" + "=" * 80)
print("æ£€æŸ¥ 6: å‰å‘ä¼ æ’­æµ‹è¯•")
print("=" * 80)

# 6.1 åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
try:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    model.eval()
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    dummy_input = torch.randn(batch_size, 3, 512, 640).to(device)
    
    # åˆ›å»º data_samples
    from mmdet.structures import DetDataSample
    from mmengine.structures import InstanceData
    data_samples = []
    for i in range(batch_size):
        ds = DetDataSample()
        ds.set_metainfo({'img_shape': (512, 640), 'modality': 'infrared'})
        gt_instances = InstanceData()
        gt_instances.bboxes = torch.tensor([[100, 100, 200, 200]], dtype=torch.float32).to(device)
        gt_instances.labels = torch.tensor([0], dtype=torch.long).to(device)
        ds.gt_instances = gt_instances
        data_samples.append(ds)
    
    print(f"\n6.1 æ¨ç†æµ‹è¯• (batch_size={batch_size}):")
    with torch.no_grad():
        outputs = model(dummy_input, data_samples, mode='predict')
    print(f"   âœ“ æ¨ç†æˆåŠŸï¼Œè¾“å‡ºæ•°é‡: {len(outputs)}")
    
    print(f"\n6.2 è®­ç»ƒæ¨¡å¼æµ‹è¯•:")
    model.train()
    losses = model(dummy_input, data_samples, mode='loss')
    print(f"   âœ“ æŸå¤±è®¡ç®—æˆåŠŸ")
    print(f"   æŸå¤±é¡¹:")
    for k, v in losses.items():
        if isinstance(v, torch.Tensor):
            print(f"     - {k}: {v.item():.4f}")
        elif isinstance(v, (list, tuple)):
            print(f"     - {k}: {[x.item() if isinstance(x, torch.Tensor) else x for x in v]}")
    
    # æ£€æŸ¥å…³é”®æŸå¤±æ˜¯å¦å­˜åœ¨
    expected_losses = ['loss_rpn_cls', 'loss_rpn_bbox', 'loss_cls', 'loss_bbox']
    for loss_name in expected_losses:
        if loss_name not in losses:
            issues.append(f"âš ï¸ ç¼ºå°‘é¢„æœŸæŸå¤±é¡¹: {loss_name}")
            print(f"   âš ï¸ ç¼ºå°‘ {loss_name}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ MACL ç›¸å…³æŸå¤±
    macl_losses = [k for k in losses.keys() if 'macl' in k.lower()]
    if not macl_losses and hasattr(model.roi_head, 'macl_head'):
        issues.append("âš ï¸ MACL æ¨¡å—å­˜åœ¨ä½†æœªäº§ç”ŸæŸå¤±")
        print(f"   âš ï¸ æœªæ£€æµ‹åˆ° MACL æŸå¤±ï¼ˆæ¨¡å—å¯èƒ½æœªæ¿€æ´»ï¼‰")
    elif macl_losses:
        print(f"   âœ“ æ£€æµ‹åˆ° MACL æŸå¤±: {macl_losses}")
    
except Exception as e:
    issues.append(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
    print(f"\n   âŒ å‰å‘æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 80)
print("é—®é¢˜æ±‡æ€»")
print("=" * 80)

if issues:
    print(f"\nå‘ç° {len(issues)} ä¸ªé—®é¢˜:\n")
    for i, issue in enumerate(issues, 1):
        print(f"{i}. {issue}")
else:
    print("\nâœ“ æœªå‘ç°æ˜æ˜¾é—®é¢˜")

print("\n" + "=" * 80)
print("å»ºè®®ä¿®å¤ä¼˜å…ˆçº§")
print("=" * 80)

critical = [i for i in issues if i.startswith('âŒ')]
warnings = [i for i in issues if i.startswith('âš ï¸')]

if critical:
    print(f"\nğŸ”´ ä¸¥é‡é—®é¢˜ ({len(critical)}):")
    for issue in critical:
        print(f"   {issue}")

if warnings:
    print(f"\nğŸŸ¡ è­¦å‘Š ({len(warnings)}):")
    for issue in warnings:
        print(f"   {issue}")

print("\n" + "=" * 80)
