"""
KAIST æ¨¡å‹å‰å‘å’Œåå‘ä¼ æ’­æµ‹è¯• (Dry Run)
éªŒè¯ï¼š
1. æ¨¡å‹å¯ä»¥æˆåŠŸæ„å»º
2. å‰å‘ä¼ æ’­æ­£å¸¸
3. æŸå¤±è®¡ç®—æ­£ç¡®
4. åå‘ä¼ æ’­æ— é”™è¯¯
5. GPU/CPU å…¼å®¹æ€§
"""
from mmengine.config import Config
from mmdet.utils import register_all_modules
from mmdet.registry import MODELS, DATASETS
import torch
from copy import deepcopy

print("=" * 80)
print("KAIST æ¨¡å‹å‰å‘-åå‘ä¼ æ’­æµ‹è¯• (Dry Run)")
print("=" * 80)

# æ£€æµ‹è®¾å¤‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
if device == 'cuda':
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æ³¨å†Œæ¨¡å—
print("\n1. æ³¨å†Œæ¨¡å—...")
register_all_modules(init_default_scope=True)
print("   âœ… å®Œæˆ")

# åŠ è½½é…ç½®
print("\n2. åŠ è½½é…ç½®...")
cfg = Config.fromfile('configs/llvip/stage2_kaist_domain_ft_nodomain.py')
print("   âœ… å®Œæˆ")

# æ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹åˆå¹¶ï¼‰
print("\n3. æ„å»ºæ¨¡å‹...")
try:
    # å°è¯•ç›´æ¥æ„å»º
    model_cfg = cfg.model.copy()
    model = MODELS.build(model_cfg)
    print("   âœ… æ¨¡å‹æ„å»ºæˆåŠŸï¼ˆç›´æ¥æ„å»ºï¼‰")
except Exception as e:
    print(f"   âš ï¸  ç›´æ¥æ„å»ºå¤±è´¥: {e}")
    print("   ğŸ”„ å°è¯•åˆå¹¶åŸºç¡€æ¨¡å‹é…ç½®...")
    
    # æ·±åº¦åˆå¹¶å‡½æ•°
    def _deep_merge(dst: dict, src: dict):
        for k, v in src.items():
            if isinstance(v, dict) and isinstance(dst.get(k), dict):
                _deep_merge(dst[k], v)
            else:
                dst[k] = v
        return dst
    
    # æ¸…ç†æœªå®ç°çš„å‚æ•°
    def _sanitize_model_cfg(m):
        m = deepcopy(m)
        roi = m.get('roi_head', {})
        for k in ['use_macl', 'use_msp', 'use_dhn', 'use_domain_loss']:
            if k in roi:
                roi.pop(k)
        m['roi_head'] = roi
        return m
    
    try:
        base_model_cfg = Config.fromfile('configs/_base_/models/faster_rcnn_r50_fpn.py')['model']
        merged_model = _deep_merge(base_model_cfg, cfg.model)
        merged_model = _sanitize_model_cfg(merged_model)
        model = MODELS.build(merged_model)
        print("   âœ… æ¨¡å‹æ„å»ºæˆåŠŸï¼ˆåˆå¹¶åŸºç¡€æ¨¡å‹ï¼‰")
    except Exception as e2:
        print(f"   âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e2}")
        raise

# ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
print(f"\n4. å°†æ¨¡å‹ç§»åŠ¨åˆ° {device}...")
model = model.to(device)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
print("   âœ… å®Œæˆ")

# æ„å»ºæ•°æ®é›†å¹¶è·å–ä¸€ä¸ªæ ·æœ¬
print("\n5. åŠ è½½æµ‹è¯•æ ·æœ¬...")
if 'test_dataloader' in cfg:
    ds_cfg = cfg.test_dataloader['dataset'] if isinstance(cfg.test_dataloader, dict) else cfg.test_dataloader.dataset
else:
    raise RuntimeError("æœªæ‰¾åˆ° test_dataloader é…ç½®")

ds_cfg = ds_cfg.copy()
ds_cfg.setdefault('return_modality_pair', False)
dataset = DATASETS.build(ds_cfg)

sample = dataset[0]
print("   âœ… æ ·æœ¬åŠ è½½æˆåŠŸ")
print(f"      - inputs shape: {sample['inputs'].shape}")

# å‡†å¤‡è¾“å…¥
print("\n6. å‡†å¤‡æ¨¡å‹è¾“å…¥...")
# é‡è¦ï¼šå›¾åƒéœ€è¦é€šè¿‡ data_preprocessor è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
# 1. å…ˆè½¬æ¢ä¸º float32 ç±»å‹
inputs_tensor = sample['inputs'].unsqueeze(0).float().to(device)  # [1, C, H, W]

# 2. æ„é€ è¾“å…¥å­—å…¸ï¼ˆæ¨¡æ‹Ÿ DataLoader çš„è¾“å‡ºæ ¼å¼ï¼‰
data_batch = {
    'inputs': [sample['inputs'].float()],  # List of tensors
    'data_samples': [sample['data_samples']]
}

# 3. ä½¿ç”¨ data_preprocessor å¤„ç†ï¼ˆè¿™ä¼šè‡ªåŠ¨è¿›è¡Œå½’ä¸€åŒ–ç­‰æ“ä½œï¼‰
if hasattr(model, 'data_preprocessor'):
    with torch.no_grad():
        data = model.data_preprocessor(data_batch, training=False)
        inputs = data['inputs']
        data_samples = data['data_samples']
    print("   âœ… å®Œæˆï¼ˆä½¿ç”¨ data_preprocessorï¼‰")
else:
    # å¦‚æœæ²¡æœ‰ data_preprocessorï¼Œä½¿ç”¨ç®€å•çš„å½’ä¸€åŒ–
    inputs = inputs_tensor / 255.0  # å½’ä¸€åŒ–åˆ° [0, 1]
    data_samples = [sample['data_samples']]
    print("   âœ… å®Œæˆï¼ˆæ‰‹åŠ¨å½’ä¸€åŒ–ï¼‰")

print(f"      - inputs shape: {inputs.shape}")
print(f"      - inputs dtype: {inputs.dtype}")
print(f"      - batch size: 1")

# æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
print("\n7. æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆæ¨ç†æ¨¡å¼ï¼‰...")
try:
    with torch.no_grad():
        results = model(inputs, data_samples, mode='predict')
    print("   âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"      - è¾“å‡ºç±»å‹: {type(results)}")
    print(f"      - è¾“å‡ºæ•°é‡: {len(results)}")
    if len(results) > 0:
        print(f"      - æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°: {len(results[0].pred_instances)}")
except Exception as e:
    print(f"   âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•æŸå¤±è®¡ç®—ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
print("\n8. æµ‹è¯•æŸå¤±è®¡ç®—ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰...")
try:
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆé‡æ–°å¤„ç†ï¼Œå› ä¸ºè®­ç»ƒæ¨¡å¼éœ€è¦ï¼‰
    train_data_batch = {
        'inputs': [sample['inputs'].float()],
        'data_samples': [sample['data_samples']]
    }
    
    # ä½¿ç”¨ data_preprocessor å¤„ç†è®­ç»ƒæ•°æ®
    if hasattr(model, 'data_preprocessor'):
        train_data = model.data_preprocessor(train_data_batch, training=True)
        train_inputs = train_data['inputs']
        train_data_samples = train_data['data_samples']
    else:
        train_inputs = sample['inputs'].unsqueeze(0).float().to(device) / 255.0
        train_data_samples = [sample['data_samples']]
    
    # è®¡ç®—æŸå¤±
    losses = model(train_inputs, train_data_samples, mode='loss')
    
    print("   âœ… æŸå¤±è®¡ç®—æˆåŠŸ")
    print(f"      - æŸå¤±é¡¹:")
    for k, v in losses.items():
        if isinstance(v, torch.Tensor):
            print(f"        {k}: {v.item():.4f}")
        else:
            print(f"        {k}: {v}")
    
    # è®¡ç®—æ€»æŸå¤±ï¼ˆé²æ£’å±•å¹³ï¼Œå…¼å®¹ list/tensorï¼‰
    def _flatten_loss_dict(loss_dict):
        flat = []
        for v in loss_dict.values():
            if isinstance(v, torch.Tensor):
                flat.append(v)
            elif isinstance(v, list):
                flat.extend([x for x in v if isinstance(x, torch.Tensor)])
        return flat
    flat_losses = _flatten_loss_dict(losses)
    total_loss = torch.mean(torch.stack([x.mean() for x in flat_losses])) if flat_losses else None
    print(f"      - æ€»æŸå¤±: {total_loss.item():.4f}")
    
except Exception as e:
    print(f"   âŒ æŸå¤±è®¡ç®—å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    total_loss = None

# æµ‹è¯•åå‘ä¼ æ’­
if total_loss is not None:
    print("\n9. æµ‹è¯•åå‘ä¼ æ’­...")
    try:
        # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
        model.zero_grad()
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦ï¼ˆæ•´æ¨¡ + è‡ªå®šä¹‰æ¨¡å—èšç„¦ï¼‰
        grad_count = 0
        none_grad_count = 0
        custom_grads = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                if param.grad is not None:
                    grad_count += 1
                    if any(x in name for x in ["macl", "msp", "alpha", "tau"]):
                        custom_grads.append((name, float(param.grad.abs().mean().item())))
                else:
                    none_grad_count += 1
        
        print("   âœ… åå‘ä¼ æ’­æˆåŠŸ")
        print(f"      - æœ‰æ¢¯åº¦çš„å‚æ•°: {grad_count}")
        print(f"      - æ— æ¢¯åº¦çš„å‚æ•°: {none_grad_count}")
        if custom_grads:
            print("      - è‡ªå®šä¹‰æ¨¡å—æ¢¯åº¦æ ·ä¾‹:")
            for n, g in custom_grads[:8]:
                print(f"        {n}: {g:.6f}")
        
    except Exception as e:
        print(f"   âŒ åå‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
else:
    print("\n9. è·³è¿‡åå‘ä¼ æ’­æµ‹è¯•ï¼ˆæŸå¤±è®¡ç®—å¤±è´¥ï¼‰")

# å†…å­˜ä½¿ç”¨ç»Ÿè®¡
if device == 'cuda':
    print("\n10. GPU å†…å­˜ä½¿ç”¨ç»Ÿè®¡...")
    allocated = torch.cuda.memory_allocated(0) / 1024**2
    reserved = torch.cuda.memory_reserved(0) / 1024**2
    print(f"   - å·²åˆ†é…: {allocated:.1f} MB")
    print(f"   - å·²ä¿ç•™: {reserved:.1f} MB")

print("\n" + "=" * 80)
print("âœ… KAIST æ¨¡å‹å‰å‘-åå‘ä¼ æ’­æµ‹è¯•å®Œæˆ")
print("=" * 80)
print("\nå¤‡æ³¨:")
print("  - å¦‚æœæŸå¤±è®¡ç®—æˆ–åå‘ä¼ æ’­å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºè‡ªå®šä¹‰æŸå¤±æœªå®ç°")
print("  - æ ‡å‡† Faster R-CNN çš„åŸºç¡€æŸå¤±ï¼ˆRPN + RoIï¼‰åº”è¯¥æ­£å¸¸å·¥ä½œ")
print("  - use_macl/use_msp/use_dhn/use_domain_loss ç­‰è‡ªå®šä¹‰é€‰é¡¹å·²è¢«ç§»é™¤")
