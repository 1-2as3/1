# Stage3 é…ç½®å‡çº§æ–¹æ¡ˆåˆ†æ

## âŒ åŸå»ºè®®çš„ä¸¥é‡é—®é¢˜

### 1. **ä¼šç ´åç°æœ‰é…ç½®ç»“æ„**
```python
# åŸå»ºè®®ä½¿ç”¨ append æ¨¡å¼
with open(stage3_cfg, 'a', encoding='utf-8') as f:
    f.write("""...""")
```

**é—®é¢˜**ï¼š
- ç›´æ¥è¿½åŠ ä¼šå¯¼è‡´é‡å¤å®šä¹‰ï¼ˆ`train_dataloader`, `model`, `param_scheduler` å·²å­˜åœ¨ï¼‰
- Python é…ç½®æ–‡ä»¶ä¸å…è®¸é‡å¤å®šä¹‰å˜é‡
- ä¼šå¯¼è‡´è¯­æ³•é”™è¯¯æˆ–åé¢çš„å®šä¹‰è¦†ç›–å‰é¢çš„

### 2. **ç¼ºå°‘å¿…éœ€çš„ pipeline é…ç½®**
```python
dataset=dict(
    type='ConcatDataset',
    datasets=[
        dict(type='LLVIPDataset', data_root='C:/LLVIP/LLVIP/', classes=('person',)),
        # âŒ ç¼ºå°‘ pipeline, ann_file, data_prefix ç­‰å¿…éœ€å­—æ®µ
    ]
)
```

**é—®é¢˜**ï¼š
- æ¯ä¸ªå­æ•°æ®é›†å¿…é¡»æœ‰å®Œæ•´çš„ `pipeline` é…ç½®
- ç¼ºå°‘ `ann_file` å¯¼è‡´æ— æ³•åŠ è½½æ ‡æ³¨
- ç¼ºå°‘ `data_prefix` å¯¼è‡´è·¯å¾„é”™è¯¯
- `classes` å‚æ•°ä¸åº”åœ¨è¿™é‡Œè®¾ç½®ï¼ˆåº”åœ¨ METAINFOï¼‰

### 3. **æŸå¤±æƒé‡é…ç½®æ–¹å¼é”™è¯¯**
```python
loss_cfg = dict(lambda_macl=0.3, lambda_dhn=0.5, lambda_domain=0.2)
```

**é—®é¢˜**ï¼š
- `loss_cfg` ä½œä¸ºç‹¬ç«‹å˜é‡ä¸ä¼šè¢« MMDet è¯»å–
- åº”è¯¥æ”¾åœ¨ `model.roi_head` å†…éƒ¨
- å‚æ•°ååº”è¯¥æ˜¯ `lambda1`, `lambda2`, `lambda3`ï¼ˆæ ¹æ®å®ç°ï¼‰

### 4. **æ¨¡å‹é…ç½®ä¸å®Œæ•´**
```python
model = dict(
    roi_head=dict(
        macl_head=dict(...),  # âŒ ç¼ºå°‘ use_macl=True
        dhn_module=dict(...)   # âŒ å‚æ•°åé”™è¯¯ï¼Œåº”è¯¥æ˜¯ dhn_cfg
    )
)
```

**é—®é¢˜**ï¼š
- ç¼ºå°‘ `use_macl`, `use_msp`, `use_dhn` å¼€å…³
- `dhn_module` åº”è¯¥æ˜¯ `macl_head` çš„å­é…ç½®
- ç¼ºå°‘ `neck` çš„ MSP é…ç½®

### 5. **ä¼šè¦†ç›–ç»§æ‰¿çš„é…ç½®**
åŸå»ºè®®è¿½åŠ çš„å†…å®¹ä¼šè¦†ç›– `read_base()` ç»§æ‰¿çš„é…ç½®ï¼Œå¯¼è‡´ï¼š
- ä¸¢å¤± backbone, neck, rpn_head é…ç½®
- ä¸¢å¤± train_cfg, test_cfg
- ä¸¢å¤±é»˜è®¤çš„ hooks å’Œ runtime è®¾ç½®

## âœ… æ­£ç¡®çš„å‡çº§æ–¹æ¡ˆ

### æ–¹æ¡ˆè®¾è®¡åŸåˆ™

1. **ä¸ç ´åç°æœ‰ç»“æ„**ï¼šä½¿ç”¨é…ç½®åˆå¹¶è€Œéè¿½åŠ 
2. **å®Œæ•´çš„ pipeline**ï¼šæ¯ä¸ªæ•°æ®é›†å¿…é¡»æœ‰å®Œæ•´é…ç½®
3. **æ­£ç¡®çš„å‚æ•°ä½ç½®**ï¼šæŸå¤±æƒé‡æ”¾åœ¨ `roi_head` å†…
4. **ä¿æŒå‘åå…¼å®¹**ï¼šç»§æ‰¿ Stage2 çš„æ‰€æœ‰æ”¹è¿›

### å®ç°æ­¥éª¤

#### Step 1: å®šä¹‰ train_pipelineï¼ˆå¿…éœ€ï¼‰

```python
# åœ¨ model å®šä¹‰ä¹‹å‰æ·»åŠ 
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', scale=(640, 640), keep_ratio=True),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 
                   'scale_factor', 'flip', 'flip_direction', 'modality')
    )
]
```

#### Step 2: æ›´æ–°æ¨¡å‹é…ç½®ï¼ˆå®Œæ•´ç‰ˆï¼‰

```python
model = dict(
    type='FasterRCNN',
    # æ·»åŠ  Neck çš„ MSP é…ç½®
    neck=dict(
        type='FPN',
        use_msp=True,
        msp_module=dict(
            type='MSPReweight',
            channels=256,
            reduction=16
        )
    ),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_head=dict(num_classes=1),
        # å¯ç”¨è‡ªå®šä¹‰æ¨¡å—
        use_macl=True,
        macl_head=dict(
            type='MACLHead',
            in_dim=256,
            proj_dim=128,
            temperature=0.07,
            use_dhn=True,
            dhn_cfg=dict(K=8192, m=0.99)  # æ­£ç¡®çš„å‚æ•°å
        ),
        use_msp=True,
        use_dhn=True,
        # æŸå¤±æƒé‡ï¼ˆæ­£ç¡®çš„ä½ç½®å’Œå‚æ•°åï¼‰
        lambda1=0.3,  # MACL æƒé‡
        lambda2=0.5,  # DHN æƒé‡
        lambda3=0.2,  # Domain æƒé‡
    )
)
```

#### Step 3: é…ç½® ConcatDatasetï¼ˆå®Œæ•´ç‰ˆï¼‰

```python
# æ–¹å¼ Aï¼šä½¿ç”¨ train_dataloaderï¼ˆMMDet 3.x æ¨èï¼‰
train_dataloader = dict(
    batch_size=2,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type='ConcatDataset',
        datasets=[
            # KAIST æ•°æ®é›†
            dict(
                type='KAISTDataset',
                data_root='C:/KAIST_processed/',
                ann_file='C:/KAIST_processed/ImageSets/train.txt',
                data_prefix=dict(sub_data_root='C:/KAIST_processed'),
                ann_subdir='Annotations',
                return_modality_pair=False,
                pipeline=train_pipeline,
            ),
            # M3FD æ•°æ®é›†
            dict(
                type='M3FDDataset',
                data_root='C:/M3FD_processed/',
                ann_file='C:/M3FD_processed/ImageSets/train.txt',
                data_prefix=dict(sub_data_root='C:/M3FD_processed'),
                ann_subdir='Annotations',
                return_modality_pair=False,
                pipeline=train_pipeline,
            ),
            # ï¼ˆå¯é€‰ï¼‰LLVIP æ•°æ®é›†
            # dict(
            #     type='LLVIPDataset',
            #     data_root='C:/LLVIP/LLVIP/',
            #     ann_file='C:/LLVIP/LLVIP/train.txt',
            #     data_prefix=dict(img_path='images'),
            #     return_modality_pair=False,
            #     pipeline=train_pipeline,
            # ),
        ]
    )
)

# æ–¹å¼ Bï¼šä½¿ç”¨ data dictï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
# data = dict(
#     train=dict(
#         type='ConcatDataset',
#         datasets=[...]  # åŒä¸Š
#     )
# )
```

## ğŸ“‹ å®Œæ•´ä¿®æ”¹è„šæœ¬

ç”Ÿæˆä¸€ä¸ªå®‰å…¨çš„ä¿®æ”¹è„šæœ¬è€Œéç›´æ¥è¿½åŠ ï¼š

```python
import os
from pathlib import Path

def upgrade_stage3_config():
    """å®‰å…¨åœ°å‡çº§ Stage3 é…ç½®"""
    stage3_path = Path(r"C:\Users\Xinyu\mmdetection\configs\llvip\stage3_joint_multimodal.py")
    
    if not stage3_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {stage3_path}")
        return
    
    # è¯»å–ç°æœ‰é…ç½®
    content = stage3_path.read_text(encoding='utf-8')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„ train_pipeline
    if 'train_pipeline = [' not in content:
        print("âš ï¸  ç¼ºå°‘ train_pipeline å®šä¹‰ï¼Œéœ€è¦æ‰‹åŠ¨æ·»åŠ ")
        return
    
    # æ£€æŸ¥ ConcatDataset æ˜¯å¦å·²é…ç½®
    if 'ConcatDataset' in content and 'pipeline=train_pipeline' in content:
        print("âœ… Stage3 é…ç½®å·²ç»åŒ…å«å®Œæ•´çš„ ConcatDataset")
        return
    
    print("ğŸ”§ éœ€è¦æ‰‹åŠ¨æ›´æ–°é…ç½®æ–‡ä»¶")
    print("   è¯·ä½¿ç”¨æä¾›çš„å®Œæ•´é…ç½®æ¨¡æ¿")

if __name__ == '__main__':
    upgrade_stage3_config()
```

## ğŸ¯ æ¨èçš„å®Œæ•´é…ç½®æ–‡ä»¶

è§ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼š`stage3_joint_multimodal_v2.py`

## âš ï¸  æ³¨æ„äº‹é¡¹

### 1. æ•°æ®é›†è·¯å¾„éªŒè¯
åœ¨ä¿®æ”¹å‰ç¡®è®¤æ‰€æœ‰è·¯å¾„å­˜åœ¨ï¼š
```bash
ls C:/KAIST_processed/ImageSets/train.txt
ls C:/M3FD_processed/ImageSets/train.txt
ls C:/LLVIP/LLVIP/train.txt  # å¦‚æœä½¿ç”¨ LLVIP
```

### 2. Pipeline ä¸€è‡´æ€§
æ‰€æœ‰å­æ•°æ®é›†åº”ä½¿ç”¨ç›¸åŒçš„ `train_pipeline`ï¼Œç¡®ä¿ï¼š
- å›¾åƒå°ºå¯¸ä¸€è‡´ï¼ˆ640x640ï¼‰
- æ•°æ®å¢å¼ºä¸€è‡´
- å½’ä¸€åŒ–å‚æ•°ä¸€è‡´

### 3. æŸå¤±æƒé‡è°ƒä¼˜
åˆå§‹æƒé‡å»ºè®®ï¼š
- `lambda1=1.0` (MACL) - è·¨æ¨¡æ€å¯¹é½æœ€é‡è¦
- `lambda2=0.5` (DHN) - å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜
- `lambda3=0.1` (Domain) - åŸŸå¯¹é½è¾…åŠ©

æ ¹æ®è®­ç»ƒæƒ…å†µè°ƒæ•´ã€‚

### 4. æ‰¹æ¬¡å¤§å°è°ƒæ•´
ConcatDataset ä¼šå¢åŠ æ•°æ®é‡ï¼Œå»ºè®®ï¼š
- GPU 16GB: `batch_size=2-4`
- GPU 8GB: `batch_size=1-2`
- ä½¿ç”¨ gradient accumulation æ¨¡æ‹Ÿæ›´å¤§ batch

### 5. å­¦ä¹ ç‡è°ƒæ•´
å¤šæ•°æ®é›†è®­ç»ƒå»ºè®®ï¼š
- åˆå§‹å­¦ä¹ ç‡ï¼š`5e-4` (å·²è®¾ç½®)
- Warmup: å‰2ä¸ªepoch
- è°ƒåº¦å™¨ï¼šCosineAnnealingï¼ˆå·²è®¾ç½®ï¼‰

## âœ… æ£€æŸ¥æ¸…å•

ä¿®æ”¹åè¿è¡Œä»¥ä¸‹æ£€æŸ¥ï¼š

- [ ] é…ç½®æ–‡ä»¶è¯­æ³•æ­£ç¡®ï¼ˆæ— é‡å¤å®šä¹‰ï¼‰
- [ ] æ‰€æœ‰æ•°æ®é›†è·¯å¾„å­˜åœ¨
- [ ] train_pipeline å®Œæ•´å®šä¹‰
- [ ] æ¨¡å‹é…ç½®åŒ…å«æ‰€æœ‰è‡ªå®šä¹‰æ¨¡å—
- [ ] æŸå¤±æƒé‡åœ¨æ­£ç¡®ä½ç½®
- [ ] è¿è¡Œ `python -m py_compile` æ£€æŸ¥è¯­æ³•
- [ ] è¿è¡Œ `python test_stage3_config.py` éªŒè¯æ„å»º

## ğŸš€ åç»­æ­¥éª¤

1. ä½¿ç”¨æä¾›çš„å®Œæ•´é…ç½®æ¨¡æ¿
2. æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
3. è¿è¡Œé…ç½®éªŒè¯è„šæœ¬
4. è®­ç»ƒå‰ç”¨å°æ•°æ®é›†æµ‹è¯•
5. ç›‘æ§ MACL/DHN/Domain æŸå¤±

---

**ç»“è®º**ï¼šåŸå»ºè®®ä¸å¯è¡Œï¼Œä¼šç ´åé…ç½®æ–‡ä»¶ã€‚åº”è¯¥ä½¿ç”¨å®Œæ•´çš„é…ç½®æ¨¡æ¿è¿›è¡Œæ›´æ–°ã€‚
